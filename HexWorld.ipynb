{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hex world problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hex world problem is a tuple of attributes:\n",
    "1. hexes: contain all tiles of the game, each tile is represent as pair of integer (coordinate in the board)\n",
    "2. reward hexes: is a subset of hexes, contain tiles which has reward (also these tiles lead to terminal state) and its correspond reward.\n",
    "3. reward border: when moving, if agent hit the wall, he will receive this \"reward border\" (usually negative as a penalty).\n",
    "4. p_intended: when moving at direction $i$, there are p_intended chance to move as intended and (1 - p_intended)/2 chance to move to two side direction $(i - 1) % 6 + 1$ and $(i + 1) % 6 + 1$.\n",
    "5. $\\gamma$: discount factor for lookahead function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct HexWorld\n",
    "    # each state is coordinate (x, y) of current hex tile\n",
    "    hexes::Vector{Tuple{Int,Int}}\n",
    "    # some state has reward \n",
    "    reward_hexes::Dict{Tuple{Int, Int}, Float64}\n",
    "    # reward if hit the wall\n",
    "    reward_border::Float64\n",
    "    # probability of successfully moving chosen direction\n",
    "    p_intended::Float64\n",
    "    # discount factor\n",
    "    γ::Float64\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Hex World"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have two example of hex world from algorithm book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: redefinition of constant general_hexworld. This may fail, cause incorrect answers, or produce other errors.\n",
      "WARNING: redefinition of constant straight_line_hexworld. This may fail, cause incorrect answers, or produce other errors.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HexWorld([(0, 0), (1, 0), (2, 0), (3, 0), (4, 0), (5, 0), (6, 0)], Dict((6, 0) => 10.0), -1.0, 0.7, 0.9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const REWARD_BORDER = -1.0 # Reward for falling off hex map\n",
    "const P_INTENDED = 0.7 # Probability of going intended direction\n",
    "const DISCOUNT_FACTOR = 0.9\n",
    "\n",
    "const general_hexworld = HexWorld(\n",
    "    [(1,0), (2,0), (3,0), (4,0), (5,0), (6,0), (7,0), (8,0), (9,0), (10,0),\n",
    "    (1,1), (2,1), (3,1), (5,1), (8,1), (9,1),\n",
    "    (0,2), (1,2), (2,2), (3,2), (5,2), (6,2), (7,2), (9,2)],\n",
    "    Dict{Tuple{Int,Int}, Float64}(\n",
    "        (1,1) =>  5.0,\n",
    "        (2,2) => -10.0,\n",
    "        (9,2) =>  10.0,\n",
    "    ),\n",
    "    REWARD_BORDER,\n",
    "    P_INTENDED,\n",
    "    DISCOUNT_FACTOR\n",
    ")\n",
    "\n",
    "const straight_line_hexworld = HexWorld(\n",
    "    [(0,0), (1,0), (2,0), (3,0), (4,0), (5,0), (6,0)],\n",
    "    Dict{Tuple{Int,Int}, Float64}(\n",
    "        (6,0) => 10.0, # right side reward\n",
    "    ),\n",
    "    REWARD_BORDER,\n",
    "    P_INTENDED,\n",
    "    DISCOUNT_FACTOR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each hex tile is consider as a state, plus one addition terminal state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each hex tile can move to six direction, numbered from 1 to 6, store in `hex_direction` array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: redefinition of constant hex_direction. This may fail, cause incorrect answers, or produce other errors.\n",
      "WARNING: redefinition of constant direction2char. This may fail, cause incorrect answers, or produce other errors.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6-element Vector{String}:\n",
       " \">\"\n",
       " \"^>\"\n",
       " \"<^\"\n",
       " \"<\"\n",
       " \"<v\"\n",
       " \"v>\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# array represent valid direction for hex tile\n",
    "const hex_direction = [(+1, 0), (+1, -1), (0, -1), (-1, 0), (-1, +1), (0, +1)]\n",
    "const direction2char = [\">\", \"^>\", \"<^\", \"<\", \"<v\", \"v>\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For state that is assigned reward, $R(s)$ is that assigned value, for other state, reward is $0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transition Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iterative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value iteration is an alternative to policy iteration that is often used because of its simplicity. Unlike policy improvement, value iteration updates the value function directly.\n",
    "\n",
    "The value function can be improved by applying the Bellman equation:\n",
    "$$U_{k+1} = \\max_a\\left(R(s,a) + \\gamma \\sum_{s'}T(s'|s,a)U_k(s')\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bellman_equation (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Base\n",
    "Base.:+(x::Tuple{Int, Int}, y::Tuple{Int, Int}) = (x[1] + y[1], x[2] + y[2])\n",
    "\n",
    "function get_Qvalue(hex::Tuple{Int,Int}, \n",
    "                    a::Int, \n",
    "                    p::Float64,\n",
    "                    hex_world::HexWorld,\n",
    "                    hex2state::Dict{Tuple{Int, Int}, Int},\n",
    "                    U::Vector{Float64})\n",
    "    next_hex = hex + hex_direction[a]\n",
    "    if !haskey(hex2state, next_hex)\n",
    "        return p * hex_world.reward_border\n",
    "    else\n",
    "        new_s = hex2state[next_hex]\n",
    "        return hex_world.γ * p * U[new_s]\n",
    "    end\n",
    "end\n",
    "\n",
    "function bellman_equation(hex_world::HexWorld, \n",
    "                            U::Vector{Float64}, \n",
    "                            hex2state::Dict{Tuple{Int, Int}, Int})\n",
    "    nS = length(hex_world.hexes)\n",
    "    nA = length(hex_direction)\n",
    "    \n",
    "    Q = zeros(nS, nA)\n",
    "    p_veer = (1.0 - hex_world.p_intended) / 2\n",
    "    p_intended = hex_world.p_intended\n",
    "    for hex in hex_world.hexes\n",
    "        s = hex2state[hex]\n",
    "        for a = 1:nA\n",
    "            if haskey(hex_world.reward_hexes, hex)\n",
    "                Q[s, a] = hex_world.reward_hexes[hex]\n",
    "            else\n",
    "                Q[s,a] += get_Qvalue(hex, a, p_intended, hex_world, hex2state, U)\n",
    "                Q[s,a] += get_Qvalue(hex, mod1(a - 1, nA), p_veer, hex_world, hex2state, U)\n",
    "                Q[s,a] += get_Qvalue(hex, mod1(a + 1, nA), p_veer, hex_world, hex2state, U)\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    new_U, new_policy = findmax(Q, dims = 2)\n",
    "\n",
    "    return vec(new_U), map(x -> x[2], new_policy)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_terminal (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using LinearAlgebra\n",
    "\n",
    "function is_terminal(U::Vector{Float64}, new_U::Vector{Float64}, threshold::Float64)\n",
    "    # println(LinearAlgebra.norm(U - new_U, 1))\n",
    "    return LinearAlgebra.norm(U - new_U, 1) <= threshold\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_optimal_policy (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function get_optimal_policy(hex_world::HexWorld, max_step::Int, threshold::Float64)\n",
    "    nS = length(hex_world.hexes)\n",
    "    U = zeros(nS)\n",
    "    policy = zeros(nS)\n",
    "    hex2state = Dict{Tuple{Int, Int}, Int}()\n",
    "    for (s, hex) in enumerate(hex_world.hexes)\n",
    "        hex2state[hex] = s\n",
    "    end\n",
    "    \n",
    "    for step = 1 : max_step\n",
    "        new_U, policy = bellman_equation(hex_world, U, hex2state)\n",
    "        # println(step, \": \", map(x -> direction2char[x], policy))\n",
    "\n",
    "        if is_terminal(U, new_U, threshold)\n",
    "            println(step)\n",
    "            break\n",
    "        end\n",
    "\n",
    "        U = new_U\n",
    "    end\n",
    "\n",
    "    return policy\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "(1, 0) v>\n",
      "(2, 0) <v\n",
      "(3, 0) <v\n",
      "(4, 0) <\n",
      "(5, 0) <\n",
      "(6, 0) >\n",
      "(7, 0) >\n",
      "(8, 0) >\n",
      "(9, 0) v>\n",
      "(10, 0) <v\n",
      "(1, 1) >\n",
      "(2, 1) <\n",
      "(3, 1) <^\n",
      "(5, 1) <^\n",
      "(8, 1) ^>\n",
      "(9, 1) v>\n",
      "(0, 2) ^>\n",
      "(1, 2) <^\n",
      "(2, 2) >\n",
      "(3, 2) <^\n",
      "(5, 2) >\n",
      "(6, 2) >\n",
      "(7, 2) ^>\n",
      "(9, 2) >\n"
     ]
    }
   ],
   "source": [
    "optimal_policy = get_optimal_policy(general_hexworld, 10000, 0.0)\n",
    "\n",
    "for (i, direction) in enumerate(optimal_policy)\n",
    "    println(general_hexworld.hexes[i], \" \", direction2char[direction])\n",
    "end\n",
    "\n",
    "# optimal_policy = get_optimal_policy(straight_line_hexworld, 100000)\n",
    "# print(optimal_policy)\n",
    "# for x in optimal_p olicy\n",
    "#     println(straight_line_hexworld.hexes[x[1]], \" \", direction2char[x[2]])\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "39cb0206baea839198ebc532c6b153ed6ed2663538cee5e413c4f91c7f10b478"
  },
  "kernelspec": {
   "display_name": "Julia 1.7.0",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
