mutable struct MGNashQLearning
    ğ’« # Markov game
    i # agent index
    Q # state-action value estimates
    N # history of actions performed
end

function MGNashQLearning(ğ’«::MG, i)
    â„, ğ’®, ğ’œ = ğ’«.â„, ğ’«.ğ’®, ğ’«.ğ’œ
    Q = Dict((j, s, a) => 0.0 for j in â„, s in ğ’®, a in joint(ğ’œ))
    N = Dict((s, a) => 1.0 for s in ğ’®, a in joint(ğ’œ))
    return MGNashQLearning(ğ’«, i, Q, N)
end

function (Ï€i::MGNashQLearning)(s)
    ğ’«, i, Q, N = Ï€i.ğ’«, Ï€i.i, Ï€i.Q, Ï€i.N
    â„, ğ’®, ğ’œ, ğ’œi, Î³ = ğ’«.â„, ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.ğ’œ[Ï€i.i], ğ’«.Î³
    ğ’¢ = SimpleGame(Î³, â„, ğ’œ, a -> [Q[j, s, a] for j in â„])
    Ï€ = solve_nash(ğ’¢)
    Ïµ = 1 / sum(N[s, a] for a in joint(ğ’œ))
    Ï€iâ€²(ai) = Ïµ/length(ğ’œi) + (1-Ïµ)*Ï€[i](ai)
    return SimpleGamePolicy(ai => Ï€iâ€²(ai) for ai in ğ’œi)
end

function update!(Ï€i::MGNashQLearning, s, a, sâ€²)
    ğ’«, â„, ğ’®, ğ’œ, R, Î³ = Ï€i.ğ’«, Ï€i.ğ’«.â„, Ï€i.ğ’«.ğ’®, Ï€i.ğ’«.ğ’œ, Ï€i.ğ’«.R, Ï€i.ğ’«.Î³
    i, Q, N = Ï€i.i, Ï€i.Q, Ï€i.N
    ğ’¢ = SimpleGame(Î³, â„, ğ’œ, aâ€² -> [Q[j, sâ€², aâ€²] for j in â„])
    Ï€ = solve_nash(ğ’¢)
    Ï€i.N[s, a] += 1
    Î± = 1 / sqrt(N[s, a])
    for j in â„
        Ï€i.Q[j,s,a] += Î±*(R(s,a)[j] + Î³*utility(ğ’¢,Ï€,j) - Q[j,s,a])
    end
end

function randstep(ğ’«::MG, s, a)
    sâ€² = rand(SetCategorical(ğ’«.ğ’®, [ğ’«.T(s, a, sâ€²) for sâ€² in ğ’«.ğ’®]))
    r = ğ’«.R(s,a)
    return sâ€², r
end

function solve_mg_nash_q_learning(ğ’«::MG, k_max)
    n_agent = length(ğ’«.â„)
    Ï€ = [MGNashQLearning(ğ’«, i) for i in ğ’«.â„] # initial joint policy
    s = rand(SetCategorical(ğ’«.ğ’®, [1.0 for s in ğ’«.ğ’®]))

    for k = 1:k_max
        a = Tuple(Ï€i(s)() for Ï€i in Ï€)
        sâ€², r = randstep(ğ’«, s, a)
        for Ï€i in Ï€
            update!(Ï€i, s, a, sâ€²)
        end
        s = sâ€²
    end
    return Ï€
end