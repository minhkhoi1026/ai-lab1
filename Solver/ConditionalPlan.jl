struct ConditionalPlan
    a # action to take at rootsubplan
    s # dictionary mapping observations to subplans
end

ConditionalPlan(a) = ConditionalPlan(a, Dict())
(Ï€::ConditionalPlan)() = Ï€.a
(Ï€::ConditionalPlan)(o) = Ï€.subplans[o]

function lookahead(ğ’«::POMDP, U, s, a)
    ğ’®, ğ’ª, T, O, R, Î³ = ğ’«.ğ’®, joint(ğ’«.ğ’ª), ğ’«.T, ğ’«.O, ğ’«.R, ğ’«.Î³
    uâ€² = sum(T(s, a, sâ€²)*sum(O(a, sâ€², o)*U(o, sâ€²) for o in ğ’ª) for sâ€² in ğ’®)
    return R(s, a) + Î³*uâ€²
end

function evaluate_plan(ğ’«::POMDP, Ï€, s)
    a = Tuple(Ï€i() for Ï€i in Ï€)
    U(o, sâ€²) = evaluate_plan(ğ’«, [Ï€i(oi) for (Ï€i, oi) in zip(Ï€, o)], sâ€²)
    return isempty(first(Ï€).subplans) ? ğ’«.R(s, a) : lookahead(ğ’«, U, s, a)
end

function utility(ğ’«::POMDP, b, Ï€)
    u = [evaluate_plan(ğ’«, Ï€, s) for s in ğ’«.ğ’®]
    return sum(bs*us for (bs,us) in zip(b, u))
end

################################################################################################

function alphavector(ğ’«::POMDP, Ï€::ConditionalPlan)
    return[evaluate_plan(ğ’«, Ï€, s) for s in ğ’«.ğ’®]
end

struct AlphaVectorPolicy
    ğ’« # POMDP problem
    Î“ # alpha vectors
    a # actions associated with alpha vectors
end

function utility(Ï€::AlphaVectorPolicy, b)
    return maximum(Î±â‹…b for Î± in Ï€.Î“)
end

function(Ï€::AlphaVectorPolicy)(b)
    i = argmax([Î±â‹…b for Î± in Ï€.Î“])
    return Ï€.a[i]
end

################################################################################################

function lookahead(ğ’«::POMDP, U, b::Vector, a)
    ğ’®, ğ’ª, T, O, R, Î³ = ğ’«.ğ’®, ğ’«.ğ’ª, ğ’«.T, ğ’«.O, ğ’«.R, ğ’«.Î³
    r = sum(R(s, a)*b[i] for (i, s) in enumerate(ğ’®))
    Posa(o, s, a) = sum(O(a, sâ€², o)*T(s, a, sâ€²) for sâ€² in ğ’®) 
    Poba(o, b, a) = sum(b[i]*Posa(o, s, a) for (i, s) in enumerate(ğ’®))
    return r + Î³*sum(Poba(o, b, a)*U(update(b, ğ’«, a, o)) for o in ğ’ª)
end

function greedy(ğ’«::POMDP, U, b::Vector)
    u, a = findmax(a->lookahead(ğ’«, U, b, a), ğ’«.ğ’œ)
    return (a = a, u = u)
end

struct LookaheadAlphaVectorPolicy
    ğ’« # POMDP problem
    Î“ # alpha vectors
end

function utility(Ï€::LookaheadAlphaVectorPolicy, b)
    return maximum(Î±â‹…b for Î± in Ï€.Î“)
end 

function greedy(Ï€, b)
    U(b) = utility(Ï€, b)
    return greedy(Ï€.ğ’«, U, b)
end

(Ï€::LookaheadAlphaVectorPolicy)(b) = greedy(Ï€, b).a

################################################################################################

function find_maximal_belief(Î±, Î“)
    m = length(Î±)
    if isempty(Î“)
        return fill(1/m, m) # arbitrary belief
    end
    model = Model(GLPK.Optimizer)
    @variable(model, Î´)
    @variable(model, b[i=1:m] â‰¥ 0)
    @constraint(model, sum(b)==1.0)
    for a in Î“
        @constraint(model, (Î±-a)â‹…b â‰¥ Î´)
    end
    @objective(model, Max, Î´)
    optimize!(model)
    return value(Î´) > 0 ? value.(b) : nothing
end

################################################################################################

function find_dominating(Î“)
    n = length(Î“)
    candidates, dominating = trues(n), falses(n)
    while any(candidates)
        i = findfirst(candidates)
        b = find_maximal_belief(Î“[i],Î“[dominating])
        if b === nothing 
            candidates[i] = false
        else
            k = argmax([candidates[j] ? bâ‹…Î“[j] : -Inf for j in 1:n])
            candidates[k], dominating[k] = false, true 
        end
    end
    return dominating
end

function prune(plans, Î“)
    d = find_dominating(Î“)
    return (plans[d],Î“[d])
end

################################################################################################

struct ValueIteration
    k_max # maximum number of iterations
end

function value_iteration(ğ’«::POMDP, k_max)
    ğ’®, ğ’œ, R = ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.R
    plans = [ConditionalPlan(a) for a in ğ’œ]
    Î“ = [[R(s, a) for s in ğ’®] for a in ğ’œ]
    plans, Î“ = prune(plans, Î“) 
    for k in 2:k_max
        plans, Î“ = expand(plans, Î“, ğ’«)
        plans, Î“ = prune(plans, Î“)
    end
    return (plans, Î“)
end

function solve(M::ValueIteration, ğ’«::POMDP)
    plans, Î“ = value_iteration(ğ’«, M.k_max)
    return LookaheadAlphaVectorPolicy(ğ’«, Î“)
end

################################################################################################

function ConditionalPlan(ğ’«::POMDP, a, plans)
    subplans = Dict(o=>Ï€ for (o,Ï€) in zip(ğ’«.ğ’ª, plans))
    return ConditionalPlan(a, subplans)
end

function combine_lookahead(ğ’«::POMDP, s, a, Î“o)
    ğ’®, ğ’ª, T, O, R, Î³ = ğ’«.ğ’®, ğ’«.ğ’ª, ğ’«.T, ğ’«.O, ğ’«.R, ğ’«.Î³
    Uâ€²(sâ€², i) = sum(O(a, sâ€², o) * Î±[i] for (o,Î±) in zip(ğ’ª, Î“o))
    return R(s, a) + Î³*sum(T(s, a, sâ€²)*Uâ€²(sâ€², i) for (i,sâ€²) in enumerate(ğ’®))
end

function combine_alphavector(ğ’«::POMDP, a, Î“o)
    return[combine_lookahead(ğ’«, s, a, Î“o) for s in ğ’«.ğ’®]
end

function expand(plans, Î“, ğ’«)
    ğ’®, ğ’œ, ğ’ª, T, O, R = ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.ğ’ª, ğ’«.T, ğ’«.O, ğ’«.R
    plansâ€², Î“â€² = [], []
    for a in ğ’œ
        # iterate over all possible mappings from observations to plans
        for inds in Iterators.product([eachindex(plans) for o in ğ’ª]...)
            Ï€o = plans[[inds...]]
            Î“o = Î“[[inds...]]
            Ï€ = ConditionalPlan(ğ’«, a, Ï€o)
            Î± = combine_alphavector(ğ’«, a, Î“o)
            push!(plansâ€², Ï€)
            push!(Î“â€², Î±)
        end
    end
    return (plansâ€², Î“â€²)
end